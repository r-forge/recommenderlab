\documentclass[fleqn, a4paper]{article}
\usepackage{a4wide}
\usepackage[round,longnamesfirst]{natbib}
\usepackage{graphicx,keyval,thumbpdf,url}
\usepackage{hyperref}
\usepackage{fancyvrb}
\usepackage{Sweave}
\SweaveOpts{strip.white=TRUE}
\AtBeginDocument{\setkeys{Gin}{width=0.6\textwidth}}

%\documentclass[article]{jss}

\usepackage[utf8]{inputenc}

%% end of declarations %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{amsmath}
\usepackage{amsfonts}


\newcommand{\strong}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\newcommand{\class}[1]{\mbox{\textsf{#1}}}
\newcommand{\func}[1]{\mbox{\texttt{#1()}}}
\newcommand{\code}[1]{\mbox{\texttt{#1}}}
\newcommand{\pkg}[1]{\strong{#1}}
\newcommand{\samp}[1]{`\mbox{\texttt{#1}}'}
\newcommand{\proglang}[1]{\textsf{#1}}
\newcommand{\set}[1]{\mathcal{#1}}
\newcommand{\sQuote}[1]{`{#1}'}
\newcommand{\dQuote}[1]{``{#1}''}
\newcommand\R{{\mathbb{R}}}
\newcommand{\mat}[1]{{\mathbf{#1}}}
\renewcommand{\vec}[1]{{\mathbf{#1}}}

\sloppy

%% \VignetteIndexEntry{An introduction to the R package recommenderlab}

\begin{document}

\title{Developing and Testing Top-$N$ Recommendation Algorithms
for 0-1 Data using \pkg{recommenderlab}
\footnote{This research was funded in part by the NSF Industry/University Cooperative Research Center for Net-Centric Software \& Systems.}
}
\author{Michael Hahsler}
\maketitle
\sloppy


\abstract{The problem of creating recommendations given a large data base from
directly elicited ratings  (e.g., ratings of 1 through 5 stars) is a popular
research area which was lately boosted by the Netflix Prize
competition.  While computing recommendations using these type of data has
direct application for example for large on-line retailers, there are many
potential applications for recommender systems where such data is not
available. However, in many cases 
there might be 0-1 rating data available or can be derived
from other data sources (e.g., purchase records, Web click data) which can be
utilized. Although this type of data differs significantly from directly
elicited ratings, only very limited research is available for
0-1 data.  This paper describes \pkg{recommenderlab} which provides the
infrastructure to test and develop recommender algorithms.  Currently the focus
is on recommender systems for 0-1 data, however in the future it can be
extended to also the more thoroughly researched case of directly elicited,
real-valued rating data.  }


<<echo=FALSE>>=
options(scipen=3, digits=4, prompt="R> ", eps=FALSE, width=75)
### for sampling
set.seed(1234)
@

\section{Introduction}
Predicting ratings and creating personalized recommendations for products like
books, songs or movies online came a long way from the first system using
\emph{social filtering} created by \cite{recommender:Malone:1987} more than 20
years ago.  Today recommender systems are an accepted technology used by market
leaders in several industries (e.g., by amazon.com, iTunes and Netflix).  
Recommender
systems apply statistical and knowledge discovery techniques to the problem of
making product recommendations based on previously recorded 
data~\citep{recommender:Sarwar:2000}.  Such
recommendations can help to improve the conversion rate by helping
the customer to find
products she/he wants to buy faster, promote cross-selling by suggesting
additional products and can improve customer loyalty through creating a
value-added relationship~\citep{recommender:Schafer:2001}.  The importance and
the economic impact of research in this field is reflected by the Netflix 
Prize~\footnote{\url{http://www.netflixprize.com/}}, a challenge to improve 
the predictions of Netflix's movie recommender system by more than 
10\% in terms
of the root mean square error. The grand price of 1 million dollar was just
awarded to the Belcore Pragmatic Chaos team.

The most widely used method to create recommendations is \emph{collaborative
filtering}. The idea is that given rating data by many users for many items
(e.g., 1 to 5 stars for movies elicited directly from the users), 
one can predict a user's rating for an item
not known to her or him~\citep[see, e.g.,][]{recommender:Goldberg:1992} 
or create
for a user a so called top-$N$ lists of recommended 
items~\citep[see, e.g.,][]{recommender:Deshpande:2004}.
For these type of recommender systems, several projects were initiated to
implement recommender algorithms (e.g., 
Apache Mahout/Taste~\footnote{\url{http://lucene.apache.org/mahout/}}, 
Cofi~\footnote{\url{http://www.nongnu.org/cofi/}}, 
%CoFE~\footnote{},
RACOFI~\footnote{\url{http://racofi.elg.ca/}}, 
SUGGEST~\footnote{\url{http://glaros.dtc.umn.edu/gkhome/suggest/overview/}}, 
Vogoo PHP~\footnote{\url{http://www.vogoo-api.com/}}).

Very limited research
is available for situations where no large amount of detailed
directly elicited rating data is available.
%~\citep[some notable exceptions are][]{recommender:Weiss:2001,recommender:Mild:2003,recommender:Lee:2005,recommender:Pan:2008}.
However, this is a common situation and occurs when
users do not want to directly reveal their preferences by rating an item 
(e.g., because it is to time consuming). In
this case preferences can only be inferred by analyzing usage behavior.
For
example, we can easily record in a supermarket setting what items a customer
purchases. However, we do not know why other products were not purchased.
The reason might be one of the following.
\begin{itemize}
\item The customer does not need the product right now.
\item The customer does not know about the product. Such a product
is a good candidate for recommendation.
\item The customer does not like the product. Such a product should 
obviously not
be recommended.
\end{itemize}

\cite{recommender:Mild:2003} and \cite{recommender:Lee:2005} 
present and evaluate 
recommender algorithms for this setting.
The same reasoning is true for
recommending pages of a web site given click-stream data. Here we only have
information about which pages were viewed but not why some pages were not
viewed. This situation leads to
binary data or more exactly to
0-1 data where 1 means that we inferred that
the user has a preference for an item and 0 means that either the user does not like the item or does not know about it. 
\cite{recommender:Pan:2008} call
this type of data in the context of collaborative filtering
analogous to similar situations for classifiers
\emph{one-class data} since only the 1-class is pure and contains only
positive examples. The 0-class is a mixture of positive and negative examples.

The \proglang{R} extension package~\pkg{recommenderlab} provides a general
infrastructure for collaborative filtering.  In this paper we will focus on the
package's capabilities for creating and testing recommender algorithms which
create top-$N$ recommendation list for 0-1 data.

%A problems that recommender systems typically have to deal with are:
%sparse data and cold start problem.

%memory-based, model-based

%Evaluation


This paper is structured as follows. Section~\ref{sec:CF} introduces
collaborative filtering and applies popular methods to the top-$N$
recommendation problem on 0-1 data.
In section~\ref{sec:evaluation} we discuss the evaluation of recommender 
algorithms.
We introduce the infrastructure provided by \pkg{recommenderlab}
in section~\ref{sec:infrastructure}. In section~\ref{sec:examples} we 
illustrate the capabilities on the package to create and evaluate
recommender algorithms. We conclude with section~\ref{sec:conclusion}.

\section{Collaborative Filtering for 0-1 Data}
\label{sec:CF}

Collaborative filtering (CF) uses given rating data by many users for many
items as the basis for predicting missing ratings and/or for creating 
a top-$N$ recommendation list for a given user, called the active user.  
Formally, we have a set of 
users $\set{U} = \{u_1, u_2, \ldots, u_m\}$ and a set of 
items $\set{I} = \{i_1, i_2, \ldots, i_n\}$.
Ratings are stored in a $m \times n$ rating matrix $\mat{R} = (r_{jk})$ where
each row represent a user $u_j$ with $1 \ge j\ge m$ and columns represent
items $i_k$ with $1 \ge k\ge n$. $r_{jk}$ represents the rating of user
$u_j$ for item $i_k$.
Typically only a small fraction of ratings are
known and for many cells in $\mat{R}$ the values are missing. Most published
algorithms operate on ratings on a specific scale (e.g., 1 to 5 (stars))
and estimated ratings are allowed to be within an interval of matching range
(e.g., $[1,5]$).
However in this
paper we concentrate on the 0-1 case with $r_{jk} \in {0,1}$ where we define:
\begin{equation}
r_{jk}=
\begin{cases}
1& \text{user $u_j$ is known to have a preference for item $i_k$} \\
0& \text{otherwise.}
\end{cases}
\end{equation}

\cite{recommender:Pan:2008} call
this type of data in the context of collaborative filtering
analogous to similar situations for classifiers
\emph{one-class data} since only the 1-class is pure and contains only
positive examples. The 0-class is a mixture of positive and negative examples.
Two strategies to deal with one-class data is to assume all missing ratings 
(zeros) are negative examples or to assume that all missing ratings are unknown.
Here will will follow mostly the first strategy based on the assumption that
users typically favor only a small fraction of the items and thus most
items with no rating will be indeed negative examples. However, it has to be
said that \cite{recommender:Pan:2008} propose strategies which represent a
trade-off between the two extreme strategies based on wighted low rank
approximations of the rating matrix and on negative example sampling which
might improve results across all recommender algorithms. 
This is however outside
the scope of this paper.


The aim of collaborative filtering is to create recommendations for a user
called the active user $u_a \in \set{U}$.  
We define the set of items unknown to user $u_a$ as 
$\set{I}_a = \set{I} \setminus \{i_l \in \set{I}| r_{al} = 1\}$.
The two typical tasks are to predict
ratings for all items in $\set{I}_a$ or to create a list of 
the best $N$ recommendations
(i.e., a top-$N$ recommendation list) for 
$u_a$~\citep{recommender:Sarwar:2001}. 
Creating a top-$N$ lists can
be seen as a second step after predicting ratings for all 
unknown items in $\set{I}_a$
and then taking the $N$ items with the highest predicted ratings.
Typically we deal with a very large number of items 
with unknown ratings
which makes first predicting rating values for all of them computationally 
expensive.
Some approaches (e.g., rule based approaches) can
predict the top-$N$ list directly without considering all unknown items first.

Formally, predicting all missing ratings is calculating a complete row of the
rating matrix $\hat{r}_{a\cdot}$ where the missing values 
for items in $\set{I}_a$ (zeros in the 0-1 case)
are replaced by ratings estimated from other data in $\mat{R}$.  
The estimated ratings can either be in $\{0,1\}$ or 
in $[0,1]$, where estimates closer to 1 indicate a stronger recommendation.
The latter type of estimation allows for ordering and thus 
is needed to be able to create a top-$N$ list.
A list
of top-$N$ recommendations for a user $u_a$ is an partially ordered set 
$T_N = (\set{X}, \ge)$, where 
$\set{X} \subset \set{I}_a$ and
$|\set{X}| \le N$ ($|\cdot|$ denotes the cardinality of the set). 
Note that there may exist cases
where top-$N$ lists contain less than $N$ items. This can happen if 
$|\set{I}_a| < N$ or if the CF algorithm is unable to 
identify $N$ items to recommend.
The binary relation $\ge$ is defined as 
$x\ge y$ if and only if 
$\hat{r}_{ax} \ge
\hat{r}_{ay}$ for all $x,y \in \set{X}$. Furthermore we 
require that $\forall_{x\in \set{X}} \forall_{y\in \set{I}_a} \quad \hat{r}_{ax} \ge \hat{r}_{ay}$ to ensure that the top-$N$ list contains
only the items with the highest estimated rating.


Collaborative filtering algorithms are typically divided into two groups,
\emph{memory-based} and \emph{model-based} 
algorithms~\citep{recommender:Breese:1998}. Memory-based algorithms
use the whole (or at least a large sample of the) user database to create 
recommendations. The most prominent algorithm is 
user-based collaborative filtering. 
The disadvantages of this approach is scalability since the whole
user database has to be processed online for creating recommendations.
Model-based algorithms 
use the user database to learn a more compact model (e.g, clusters
with users of similar preferences) that is later used to create
recommendations.

In the following we will present well known memory and model-based
collaborative filtering algorithms and apply them to 0-1 data.

\subsection{User-based Collaborative Filtering}

User-based CF~\citep{recommender:Goldberg:1992,Resnick:1994,recommender:Shardanand:1995} is
a memory-based algorithm which tries to mimics word-of-mouth based on analysis
of rating data. The assumption is that 
users with similar preferences will rate products similarly. Thus
missing ratings for a user can be predicted by first 
finding a \emph{neighborhood} of similar users and then aggregate the 
ratings of these users to form a prediction.

The neighborhood is defined in terms of similarity between users,
either by taking a given number of most similar users ($k$ nearest neighbors) 
or all users within
a given similarity threshold.
Popular similarity measures for CF are
the \emph{Pearson correlation coefficient} and
the \emph{Cosine similarity}. These similarity measures are defined
between two users $u_x$ and $u_y$ as
$$\mathrm{sim_{Pearson}}(\vec{x},\vec{y}) = 
	    \frac{\sum_{i \in I} (\vec{x}_i \, \bar{\vec{x}})(\vec{y}_i \, \bar{\vec{y}})}
		{(|I| -1) \, \mathrm{sd}(\vec{x}) \, \mathrm{sd}(\vec{y})}
$$

and
$$\mathrm{sim_{Cosine}}(\vec{x},\vec{y}) = 
	    \frac{\vec{x}\cdot\vec{y}}
		    {\|\vec{x}\|_2\|\vec{y}\|_2},$$
where
$\vec{x} = r_{x\cdot}$ and 
$\vec{y} = r_{y\cdot}$ represent the users' profile vectors.
$\mathrm{sd}(\cdot)$ is the standard deviation and
$\|\cdot\|_2$ is the $l^2$-norm.
For calculating similarity using rating data only the dimensions (items) 
are used which were rated by both users. However, for 0-1 data that would lead
to the problem that the vectors $\vec{x}$ and $\vec{y}$ only contain ones
and thus no useful measure can be calculated. 

If we assume that most zeroes are actually items that the user does not like,
we can use all items in the similarity calculation. However, this will produce
significant errors for newer users with very few ones. 
A similarity measure which
only focuses on matching ones 
and thus prevents the problem with zeroes
is the \emph{Jaccard index}:
$$\mathrm{sim_{Jaccard}}(\set{X},\set{Y}) = \frac{|\set{X}\cap \set{Y}|}
{|\set{X}\cup \set{Y}|},$$ 
where
$\set{X}$ and $\set{Y}$ are the sets of the items with a 1 in user profiles
$u_a$ and $u_b$, respectively.

Now the neighborhood $\set{N} \subset \set{U}$ can be selected 
by either a threshold on the similarity or by taking the
$k$ nearest neighbors.
Once the users in the neighborhood are found, their ratings are  
aggregated to form the predicted rating for the active user.
For real valued ratings, \cite{recommender:Breese:1998} suggest
to aggregate the ratings for item $i_j$ as
$$\hat{r}_{aj}=\bar{r}_a + \kappa \sum_{i\in\set{N}}{w_{ai} 
(r_{ij}-\bar{r}_i)}$$
where $\bar{r}_x$ is the mean of the ratings of user $u_x$, 
$w_{ai}$ is the weight for user $u_i$ and
$\kappa$ is
a normalizing factor to make the weights sum to 1.
The weights can reflect the similarity between the user and the active user.
For 0-1 data we suggest, following \cite{recommender:Weiss:2001}, 
to calculate the following score.
$$s_{aj} = \sum_{i\in\set{N}}{w_{ai}\, r_{ij}}$$
This score is not normalized but can be easily used to find the
top-$N$ items with the highest score.

\begin{figure}
\centerline{\includegraphics[scale=1]{user-based}}
\caption{User-based collaborative filtering.}
\label{fig:UBCF}
\end{figure}

%% Weiss suggests vote spliting and a simple similarity measure.

An example of the process of creating recommendations for 0-1 data by
user-based CF is shown in Figure~\ref{fig:UBCF}. To the left is the rating
matrix $\mat{R}$ with 6 users and 8 items. The active user $u_a$ we want to
create recommendations for is shown at the bottom of the matrix.  To find the
$k$-neighborhood (i.e., the $k$ nearest neighbors) we calculate the similarity
between the active user and all other users in the database and then select the
$k$ users with the highest similarity.  To the right in Figure~\ref{fig:UBCF}
we see a $2$-dimensional representation of the similarities (users with higher
similarity are closer) with the active user in the center. The $k=3$ nearest
neighbors are selected and marked in the database to the left. To generate an
aggregated score, we use for the example a weight of 1 for all users.
Thus the ones in the selected users are just summed up. Then items known
to the active user are removed and the $N$ items with the highest score
(greater than zero) form the top-$N$ recommendations. In the example in 
Figure~\ref{fig:UBCF} only two items are recommended.

The two main problems of user-based CF are that the whole
user database has to be kept in memory and that 
expensive similarity computation between the active user and
all other users in the database has to be performed.


\subsection{Item-based Collaborative Filtering}
Item-based CF~\citep{Kitts:2000,recommender:Sarwar:2001,recommender:Linden:2003,recommender:Deshpande:2004}
is a model-based approach which produces recommendations
based on the relationship between items inferred from the rating
matrix. The assumption behind this approach is that users will prefer items that
are similar to the items they like.

The model-building step consists of calculating a similarity matrix
containing all item-to-item similarities using a given similarity measure.
Popular are again Pearson correlation and Cosine similarity. For 0-1
data again the Jaccard index can be used giving focus to matching ones.
For item-based CF, \cite{recommender:Deshpande:2004} proposed
a \emph{Conditional probability-based 
similarity} defined as:
	$$\mathrm{sim_{Conditional}}(x,y) = 
	    \frac{\mathrm{Freq}(xy) }
		    {\mathrm{Freq}(x)} = \hat{P}(y|x),$$
where $x, y \in \set{I}$ are two items and
$\mathrm{Freq(\cdot)}$ is the number of users with the given items in their
profile. This similarity is in fact an estimate of the conditional
probability to see item $y$ in a profile given that the profile contains
items $x$. This similarity is equivalent to the confidence measure used
for association rules (see section~\ref{sec:AR} below). A drawback
of this similarity measure is its sensitivity to the frequency of $x$ with
rare $x$ producing high similarities.
To reduce the sensitivity, \cite{recommender:Deshpande:2004} propose a
normalized version of the similarity measure:
	$$\mathrm{sim_{Karypis}}(x,y) = 
	    \frac{\sum_{\forall i b_{i,x}} b_{i,y} }
	    {\mathrm{Freq}(x) \mathrm{Freq}(y)^\alpha}$$
where $\mathbf{B} = (b_{i,j})$ 
is a normalized rating matrix where all rows sum up to $1$.
$\mathrm{Freq}(y)^\alpha$ reduces the problem with rare $x$.

All pairwise similarities are stored in a $N \times N$ 
similarity matrix $\mat{S}$,
which is again normalized such that rows sum up to $1$.
To reduce the model size
to $N \times k$ with $k \ll N$, 
for each item only a list of the $k$ most similar
items and their similarity values are stored~\citep{recommender:Sarwar:2001}.
This can improve the
space and time complexity significantly by sacrificing 
some recommendation quality.

Figure~\ref{fig:IBCF} shows an example for $N=8$ items. For the 
normalized similarity matrix $\mat{S}$ only $k=3$ entries are
stored per row (the crossed out entries are discarded).

\begin{figure}
\centerline{\includegraphics[scale=1]{item-based2}}
\caption{Item-based collaborative filtering}
\label{fig:IBCF}
\end{figure}

To make a recommendation based on the model only two steps are necessary: 
\begin{enumerate}
\item Calculate a score for each item by adding the similarities 
with the active user's items.
\item Remove the items of the active user and recommend the $N$ items 
with the highest score.
\end{enumerate}

In Figure~\ref{fig:IBCF} we assume that the active user prefers items
$i_1, i_5$ and $i_8$. The rows corresponding to these items are highlighted
and the rows are added up. Now the sums for the items 
preferred by the user are removed (crossed out in Figure~\ref{fig:IBCF}),
leaving three items with a score larger than zero which results in 
the top-$N$ recommendation list $i_3, i_6, i_4$.


Item-based CF are very efficient since the
models (reduced similarity matrix) is relatively small ($N \times k$) and
can be fully precomputed. Item-based CF is known to 
only produce slightly inferior results compared to user-based 
CF and higher order models 
which take the joint distribution of
sets of items into account 
are possible~\citep{recommender:Deshpande:2004}.
Furthermore, item-based CF is successfully applied 
in large scale recommender systems (e.g., by Amazon.com).

\subsection{Association Rules}
\label{sec:AR}
Recommender systems based on association rules
produce recommendations based on a dependency model for items 
given by a set of association 
rules~\citep{Fu:2000,Mobasher:2001,Geyer-Schulz:2002,Lin:2002,Demiriz:2004}. 
The binary profile matrix $\mat{R}$ is seen as a database 
where each user is treated as a transaction that contains
the subset of items in $\set{I}$ with a rating of 1.
Hence transaction $k$ is defined as 
$\set{T}_k = \{i_j \in \set{I} | r_{jk} = 1\}$ and
the whole transaction data base is 
$\set{D} = \{\set{T}_1, \set{T}_2, \ldots, \set{T}_U\}$ where $U$ is the number of users.
To build the dependency model, 
a set of association rules $\set{R}$ is mined from
$\mat{R}$. Association rules are rules of the form
$\set{X} \rightarrow \set{Y}$  where $\set{X}, \set{Y} \subseteq \set{I}$
and $\set{X} \cap \set{Y} = \emptyset$.
For the model we only use association rules with a single item in
the right-hand-side of the rule ($|\set{Y}| = 1$).
To select a set of useful association rules, 
thresholds on measures of significance and interestingness are used. Two widely applied measures are:
\begin{equation*}
\mathrm{support}(\set{X} \rightarrow \set{Y}) = 
\mathrm{support}(\set{X} \cup \set{Y}) = 
\mathrm{Freq}(\set{X} \cup \set{Y}) / |\set{D}|
\end{equation*}
\begin{equation*}
\mathrm{confidence}(\set{X} \rightarrow \set{Y}) = \mathrm{support}(\set{X} \cup \set{Y}) / \mathrm{support}(\set{X}) = \hat{P}(\set{Y}|\set{X}) 
\end{equation*}

$\mathrm{Freq}(\set{X})$ gives the number of transactions
in the data base $\set{D}$ that contains all items in $\set{X}$.

We now require $\mathrm{support}(\set{X} \rightarrow \set{Y}) > s$ and 
$\mathrm{confidence}(\set{X} \rightarrow \set{Y}) > c$ 
and also include a length constraint $|\set{X} \cup \set{Y}|\leq l$.
The set of rules $\set{R}$ that satisfy these constraints form the dependency
model. Although finding all association rules given thresholds
on support and confidence is a hard problem (the model grows in the 
worse case exponential with the number of items), algorithms that efficiently
find all rules in most cases are 
available~\citep[e.g.,][]{arules:Agrawal:1994,arules:Zaki:2000,arules:Han:2004}. Also model size can be controlled by $l$, $s$ and $c$.


To make a recommendation for an active user $u_a$ given the
set of items $\set{T}_a$ the user likes and the
set of association rules $\set{R}$ (dependency model),
the following steps are necessary:
\begin{enumerate}
\item Find all matching rules $\set{X} \rightarrow \set{Y}$ for 
which $\set{X} \subseteq \set{T}_a$ 
in $\set{R}$.
\item Recommend $N$ unique right-hand-sides ($\set{Y}$) of the matching rules 
with the highest confidence (or another measure of interestingness).
\end{enumerate}

The dependency model is very similar to
item-based CF with conditional probability-based 
similarity~\citep{recommender:Deshpande:2004}. It can be fully precomputed
and rules with more than one items in the left-hand-side ($\set{X}$), 
it incorporates higher order effects between more than two items. 
%Mbasher et al [Mobasher et al. 2000] presented an algorithm for recommending
%additional webpages to be visited by a user based on association rules. In this
%approach, the historical information about users and their web-access patterns
%were mined using a frequent itemset discovery algorithm and were used to
%generate a set of high con- fidence association rules. The recommendations were
%computed as the union of the consequent of the rules that were supported by the
%pages visited by the user. Lin et al [Lin et al. 2000] used a similar approach
%but they developed an algorithm that is guaranteed to find association rules
%for all the items in the database. Finally, within the context of using
%association rules to derive top-N recommendations, Demiriz [Demiriz 2001]
%studied the problem of how to weight the different rules that are supported
%by the active user. He presented a method that computes the similarity
%between a rule and the active user¿s basket as the product of the confi-
%dence of the rule and the Euclidean distance between items in the
%antecedent of the association rule and the items in the user¿s basket. He
%compared this approach both with the item-based scheme described in Section
%4 (based on our preliminary work presented in [Karypis 2001]) and the
%dependency network-based algorithm [Heckman et al. 2000].


\section{Evaluation of Top-$N$ Recommender Algorithms for 0-1 Data}
\label{sec:evaluation}

Evaluation of recommender systems is an important topic but
most evaluation efforts center on recommender systems for
non-binary rating data. A comprehensive review was presented 
by \cite{recommender:Herlocker:2004}.
However, here we will discuss the evaluation of top-$N$ recommender
algorithms for 0-1 data.

Typically, 
given a rating matrix $\mat{R}$,
recommender algorithms are evaluated by first
partitioning the users (rows) in $\mat{R}$ into two 
sets $\set{U}_\mathit{train}\, \cup\, \set{U}_\mathit{test} = \set{U}$.
The rows of $\mat{R}$ corresponding to the training users $U_\mathit{train}$ 
are used to learn the recommender model.
Then each user $u_a \in \set{U}_\mathit{test}$ is seen as an active user,
however,
before creating recommendations some items are withheld from the
profile $r_{u_a\cdot}$ and it measured how well these removed items are
predicted by the recommender algorithm in its top-$N$ list.
This type of evaluation does not take into account that $0$ in the
data also codes for items that are unknown to the user and could 
potentially be a good recommendation. However, it is assumed that if
a recommender algorithm performed better in predicting the withheld items,
it will also perform better in finding good recommendations for unknown items.

To determine how to split $\set{U}$ into 
$\set{U}_\mathit{train}$ and $\set{U}_\mathit{test}$ we can use 
several approaches~\citep{recommender:Kohavi:1995}. 
\begin{itemize}
\item {\bf Splitting:} 
We can randomly assign a predefined proportion of the users 
to the training set and all others to the test set.
\item {\bf Bootstrap sampling:}
We can sample from $\set{U}_\mathit{test}$ with replacement
to create the training set and then use the users not in the training set as
the test set. This procedure has the advantage that for smaller data sets
we can create larger training sets and still have users left for testing.
\item {\bf $k$-fold cross-validation:} Here we split $\set{U}$ into $k$ sets
(called folds) of approximately the same size. Then we evaluate $k$ times,
always using one fold for testing and all other folds for leaning. The $k$
results can be averaged. This approach makes sure that each user is at least
once in the test set and the averaging produces more robust results and error
estimates.
\end{itemize}

The items withheld in the test data are randomly chosen.
\cite{recommender:Breese:1998} introduced the four experimental protocols
called {\em Given 2}, {\em Given 5}, {\em Given 10} and {\em All but 1}. For
the Given $x$ protocols for each user $x$ randomly chosen items are given to
the recommender algorithm and the remaining items are withheld for evaluation.
For All but $x$ the algorithm gets all but $x$ withheld items.

The prediction results for all test users $\set{U}_{test}$ 
can be aggregated into a so called {\em confusion matrix} depicted in
table~\ref{tab_confusion} (see \cite{recommender:Kohavi:1998}) 
which corresponds
exactly to the outcomes of a classical statistical experiment.
The confusion matrix shows how many of the items recommended in
the top-$N$ lists (column predicted positive; $d+b$) 
were withheld items and thus correct recommendations
(cell $d$) and how many where potentially incorrect (cell $b$). 
The matrix also shows
how many of the not recommended items
(column predicted negative; $a+c$) 
should have actually been recommended since they represent withheld items (cell $c$). 

\begin{table}[tbp]
\caption{2x2 confusion matrix
\label{tab_confusion}
}
\center
\begin{tabular}{|c||c|c|} 
\hline
{\bf actual / predicted} & {\bf negative}  & {\bf positive} \\
\hline
\hline
{\bf negative} & $a$ & $b$ \\
\hline
{\bf positive} & $c$ & $d$ \\
\hline
\end{tabular}
\end{table}

From the confusion matrix several performance measures can be derived.
For the data mining task of a recommender system 
the performance of an algorithm depends on its ability
to learn significant patterns in the data set.
Performance measures used to evaluate these algorithms 
have their root in machine learning.
A commonly used measure is
{\em accuracy,} the fraction of correct 
recommendations to total possible recommendations.

\begin{equation} 
\mathit{Accuracy} = \frac{\mathit{correct\ recommendations}}{\mathit{total\ possible\ recommendations}}
=  \frac{a+d}{a+b+c+d}
\label{accur}
\end{equation} 

A common error measure is the {\em mean absolute error} ({\em MAE}, also called {\em mean absolute deviation} or {\em MAD}).
\begin{equation} 
\mathit{MAE} = \frac{1}{N}\sum_{i=1}^N{|\epsilon_i|} = \frac{b+c}{a+b+c+d},
\label{mae}
\end{equation} 
where $N = a+b+c+d$ is the total number of items 
which can be recommended and $|\epsilon_i|$ is the
absolute error of each item.
Since we deal with 0-1 data, $|\epsilon_i|$ can only be zero (in cells $a$ and $d$ in the confusion matrix) or one (in cells $b$ and $c$).
For evaluation recommender algorithms for rating data, the root mean square 
error is often used. For 0-1 data it reduces to the square 
root of MAE.  

Recommender systems help to find items of interest from the set of all 
available items. This can be seen as a retrieval task known from 
information retrieval. Therefore, standard information retrieval
performance measures are frequently used to evaluate recommender performance.
{\em Precision} and {\em recall} are the best known measures used in 
information retrieval \citep{recommender:Salton:1983,recommender:Rijsbergen:1979}. 

\begin{equation} 
\mathit{Precision} = \frac{\mathit{correctly\ recommended\ items}}{\mathit{total\ recommended\ items}}
= \frac{d}{b+d}
\label{prec}
\end{equation} 

\begin{equation} 
\mathit{Recall} = \frac{\mathit{correctly\ recommended\ items}}{\mathit{total\ useful\ recommendations}}
= \frac{d}{c+d}
\label{rec}
\end{equation} 

%% fallout, generality

Often the number of {\em total\ useful\ recommendations} needed for recall is
unknown since the whole collection would have to be inspected. However, instead
of the actual {\em total\ useful\ recommendations} often the total number of
known useful recommendations is used.  Precision and recall are
conflicting properties, high precision means low recall and vice versa. To find
an optimal trade-off between precision and recall a single-valued measure like
the {\em E-measure} \citep{recommender:Rijsbergen:1979} can be used.  
The parameter $\alpha$ controls the
trade-off between precision and recall.
\begin{equation} 
\mbox{\textit{E-measure}} = \frac{1}{\alpha(1/\mathit{Precision}) + (1-\alpha)(1/\mathit{Recall})}
\label{e-measure}
\end{equation} 

A popular single-valued measure is the {\em F-measure.} It is defined as the
harmonic mean of precision and recall.  
\begin{equation} 
\mbox{\textit{F-measure}} = \frac{2\, \mathit{Precision}\, \mathit{Recall}}{\mathit{Precision} + \mathit{Recall}} = 
\frac{2}{1/\mathit{Precision} + 1/\mathit{Recall}}
\label{f-measure}
\end{equation} 
It is
a special case of the E-measure with $\alpha=.5$ which places the same weight
on both, precision and recall.  In the recommender evaluation literature the
F-measure is often referred to as the measure {\em F1.}

Another method used in the literature to compare two classifiers at different 
parameter settings is the {\em Receiver Operating Characteristic (ROC)}.  
The method was developed for signal detection and goes back to the Swets model
\citep{recommender:Rijsbergen:1979}. The ROC-curve is a plot of the system's
{\em probability of detection} (also called $\mathit{sensitivity}$ or true
positive rate TPR which is equivalent to recall as defined in 
formula \ref{rec}) by the {\em probability of false alarm} 
(also called false positive rate FPR or
$1-\mathit{specificity}$, where
$\mathit{specificity} = \frac{a}{a+b}$) with
regard to model parameters.  A possible way to compare the efficiency of two
systems is by comparing the size of the area under the ROC-curve, where a
bigger area indicates better performance. 
%% \cite{recommender:Schein2005} for CROC



\section{Recommenderlab Infrastructure}
\label{sec:infrastructure}
\pkg{recommenderlab} is implemented using formal classes in 
the \proglang{S4} class system. 
Figure~\ref{fig:class_diagram} shows the main classes and their relationships. 

\begin{figure}
\centerline{\includegraphics[width=0.8\textwidth]{recommenderlab_classes}}
\caption{UML class diagram for 
package~\pkg{recommenderlab}~\citep{misc:Fowler:2004}.}
\label{fig:class_diagram}
\end{figure}

The package uses the abstract \class{ratingMatrix} 
to provide a common interface for rating data. \class{ratingMatrix}
implements many methods typically available for matrix-like
objects. For example, \func{dim}, \func{dimnames}, 
\func{colCounts}, \func{rowCounts},
\func{colMeans}, \func{rowMeans},
\func{colSums} and \func{rowSums}.
Additionally \func{sample} can be used to sample from users (rows) and
\func{image} produces an image plot.

For \class{ratingMatrix} we provide two concrete 
implementations \class{realRatingMatrix} and 
\class{binaryRatingMatrix} to represent the rating matrix $\mat{R}$.
\class{binaryRatingMatrix} implements a 0-1 rating matrix using
the implementation of \class{itemMatrix} defined in package~\pkg{arules}. 
\class{itemMatrix} stores only the ones and internally uses a sparse
representation from package \pkg{Matrix}. Although the focus of the package and
this paper is on 0-1 data, for completeness the package contains class
\class{realRatingMatrix} which implements a rating matrix with real valued 
ratings stored in sparse format defined in package \pkg{Matrix}. With this 
class the infrastructure of \pkg{recommenderlab} can be easily extended
to non-binary data.

Class \class{Recommender} implements the data structure to store recommendation
models. The creator method 

\begin{center}
\code{Recommender(data, method, parameter = NULL)} 
\end{center}

takes data as a \class{ratingMatrix},
a method name and some optional parameters for the method and
returns a \class{Recommender} object. Once we have a recommender object,
we can predict top-$N$ recommendations for active users using

\begin{center}
\code{predict(object, newdata, n=10, ...)}. 
\end{center}

\code{object} is the recommender
object, \code{newdata} is the data for the active users and \code{n} is
the number of recommended items $N$ in each top-$N$ list. \func{predict} will
return a list of objects of class \class{topNList}, one for each 
of the active users in \code{newdata}. 

The actual implementations for the recommendation algorithms are 
managed using the registry mechanism provided by package \pkg{registry}.
%The registry called \code{recommenderRegistry} uses the four fields
%\code{method} (name of the recommendation algorithm), \code{dataType} (name of
%the concrete implementation of \class{ratingMatrix} the algorithm works on),
%\code{fun} (function to create a recommender model as an instance of class
%\class{Recommender}), and \code{description} (verbal description of the
%algorithm) to describe a recommender algorithm. 
Generally, the registry mechanism is hidden from the user and the
creator function \func{Recommender} uses it in the background to map 
a recommender method name to its implementation. However, the 
registry can be directly queried and new recommender algorithms can be added by the user. We will give and example for this feature 
in the examples section of this paper.

To evaluate recommender algorithms package~\pkg{recommenderlab}
provides the infrastructure to create and maintain evaluation schemes 
stored as an object of class \class{evaluationScheme} from rating data. 
The creator function 

\begin{center}
\code{evaluationScheme(data, method="split", train=0.9, k=10, given=3)} 
\end{center}

creates the evaluation scheme from a data set using a method
(e.g., simple split, bootstrap sampling, $k$-fold cross validation)
with item withholding (parameter \code{given}). 
The function \func{evaluate} is then used to
evaluate several recommender algorithms using an evaluation scheme resulting
in a evaluation result list (class~\class{evaluationResultList}) with
one entry (class~\class{evaluationResult}) per algorithm. 
Each object of \class{evaluationResult} 
contains one or several object of \class{confusionMatrix} depending on the 
number of evaluations specified in the \class{evaluationScheme} (e.g., $k$
for $k$-fold cross validation).
With this infrastructure several recommender algorithms can be compared
on a data set with a single line of code.

In the following, we will illustrate the usage of \pkg{recommenderlab}
with several examples.
\section{Examples}
\label{sec:examples}
\subsection{A first session}

This fist example shows how to train and
use a recommender algorithm.
For the example we use the data set \emph{MSWeb} which is included in
\pkg{recommenderlab}. 
The data set contains anonymous web click-stream data 
from \url{www.microsoft.com} 
for 38,000 anonymous, randomly selected users. For each
user, the data lists all the areas of the web site (called Vroots; 
e.g., MS Office, Windows NT Server) 
that the user visited
in a one week time frame \citep{recommender:Breese:1998}.

We first load the package and the data set and then select for the
example only users who visited more than 5 areas.
Since the areas/items are columns in the 0-1 rating matrix, we select all 
rows/users with a row count larger than 5.

<<>>=
library("recommenderlab")
data(MSWeb)

MSWeb5 <- MSWeb[rowCounts(MSWeb) >5,]
MSWeb5
@

The used data set contains \Sexpr{nrow(MSWeb5)} users with more 
than $5$ items and
\Sexpr{ncol(MSWeb5)} items.  
To understand the distribution of the number of
areas/items each user visited, we can produce a histogram.

<<rowCounts, fig=TRUE, include=FALSE, height=4>>=
hist(rowCounts(MSWeb5), breaks=20)
@

\begin{figure}
\centerline{\includegraphics[scale=1]{recommenderlab-rowCounts}}
\caption{Distribution of the number of items per user (rowCounts).}
\label{fig:rowCounts}
\end{figure}

Since the rows in the rating matrix represent the users, the histogram in
Figure~\ref{fig:rowCounts} shows the distribution of the users with $6, 7,
\ldots$ items (note that we required users to have at least 5 items).
On average a user has \Sexpr{round(mean(rowSums(MSWeb5)), 2)}
items in her/his profile.

The profile of users can be inspected with \func{LIST}. 
For example, we can see what areas the first two users visited. 

<<>>=
LIST(MSWeb5[1:2])
@

It is also interesting to study the popularity of items.
To get a first idea, we use an image plot
for 200 randomly chosen users.

<<eval=FALSE>>=
image(sample(MSWeb5, 200))
@
<<image, fig=TRUE, include=FALSE, echo=FALSE>>=
plot(image(sample(MSWeb5, 200)))
@

\begin{figure}
\centerline{\includegraphics[scale=1]{recommenderlab-image}}
\caption{Image plot of 200 randomly chosen users in the rating matrix $\mat{R}$.
Dark squares represent 1s in the matrix.}
\label{fig:image}
\end{figure}


Figure~\ref{fig:image} shows that some items are 
by far more popular than others. We can further 
look at the
distribution of how many user profiles contain an item. 
This is done by plotting a histogram for the column sums of the rating matrix.

<<colCounts, fig=TRUE, include=FALSE, height=4>>=
hist(colCounts(MSWeb5), breaks=25)
@

\begin{figure}
\centerline{\includegraphics[scale=1]{recommenderlab-colCounts}}
\caption{Distribution of item popularity (colCounts).}
\label{fig:colCounts}
\end{figure}

The histogram is shown in Figure~\ref{fig:colCounts}.
We see a typical distribution where 
the number of items falls quickly with popularity and 
only very few items are extremely popular (tail of the distribution).

A recommender is created using the creator function \func{recommender}.
Here we create a simple recommender which generates recommendations 
solely on the popularity of items (the number of users who have the item in
their profile). We create a recommender using the first 1000 users in the
data set.

<<>>=
r <- Recommender(MSWeb5[1:1000], method = "POPULAR")
r
@

The model can be obtained from a recommender using \func{getModel}.
<<>>=
getModel(r)
@

In this case the model is just a short description and a simple 
vector called \code{popOrder} containing the order of items
according to popularity in the data set.

Recommendations are generated by \func{predict} in the same way predict
is used in \proglang{R} for other types of models. The result 
are recommendations in the form of an object of class~\class{TopNList}.
Here we create top-5 recommendation lists for two
users who were not used to learn the model.

<<>>=
recom <- predict(r, MSWeb5[1001:1002], n=5)
recom
@

The result are two ordered top-$N$ recommendation lists,
one for each user. The recommended items can be inspected 
using \func{LIST}.
<<>>=
LIST(recom)
@

Since the top-$N$ lists are ordered, we can extract sublists of
the best items in the top-$N$. For example, we can get the best 3 
recommendations for each list using \func{bestN}.
<<>>=
recom3 <- bestN(recom, n = 3)
recom3
LIST(recom3)
@

Next we will look at the evaluation of recommender algorithms.

\subsection{Evaluation of a recommender algorithm}

\pkg{recommenderlab} implements several 
standard evaluation methods for recommender systems. 
Evaluation starts with creating an evaluation scheme
that determines what and how data is used for training and evaluation.
Here we create a $4$-fold cross validation scheme 
with the the Given-3 protocol, i.e.,
for the test users all but three randomly selected items are withheld 
for evaluation.

<<>>=
scheme <- evaluationScheme(MSWeb5, method="cross", k=4, given=3)
scheme
@

Next we use the created evaluation scheme to evaluate the recommender 
method popular.
We evaluate top-1, top-3, top-5, top-10, top-15 and top-20 recommendation lists.

<<>>=
results <- evaluate(scheme, method="POPULAR", n=c(1,3,5,10,15,20))
results
@

The result is an object of class~\class{EvaluationResult} which
contains several confusion matrices. \func{getConfusionMatrix} 
will return the confusion matrices for the 4 runs 
(we used 4-fold cross evaluation) as a list.
In the following we look at the first element of the list which
represents the first of the 4 runs.


<<>>=
getConfusionMatrix(results)[[1]]
@

For the first run we have 6 confusion matrices represented by rows, one for each
of the six different top-$N$ lists we used for evaluation.
$n$ is the number of recommendations per list. TP, FP, FN and TN are the
entries for
true positives, false positives, false negatives and true negatives in 
the confusion matrix. The remaining columns contain precomputed performance 
measures. The average for all runs can be obtained 
from the evaluation results directly using \func{avg}.

<<>>=
avg(results)
@

Evaluation results can be plotted using \func{plot}. The default
plot is the ROC curve which plots the true positive rate (TPR) against the 
false positive rate (FPR). 
<<roc1, fig=TRUE, include=FALSE>>=
plot(results, annotate=TRUE)
@
\begin{figure}
\centerline{\includegraphics[scale=1]{recommenderlab-roc1}}
\caption{ROC curve for recommender method POPULAR.}
\label{fig:roc1}
\end{figure}

For the plot where we annotated the curve with the size of the top-$N$ list
is shown in Figure~\ref{fig:roc1}.
By using \code{"prec/rec"} as the second argument, a precision-recall plot
is produced (see Figure~\ref{fig:precrec1}).

<<precrec1, fig=TRUE, include=FALSE>>=
plot(results, "prec/rec", annotate=TRUE)
@
\begin{figure}
\centerline{\includegraphics[scale=1]{recommenderlab-precrec1}}
\caption{Precision-recall plot for method POPULAR.}
\label{fig:precrec1}
\end{figure}

\subsection{Comparing recommender algorithms}

The comparison of
several recommender algorithms is one of the main functions of 
\pkg{recommenderlab}. For comparison also \func{evaluate} is used.
The only change is to use \func{evaluate} with
a list of algorithms together with their parameters instead of 
a single method name. In the following we use the
evaluation scheme created above to compare the five 
recommender algorithms: random items, popular items, 
user-based CF, item-based CF, and association rule based recommendations.
Note that when running the following code, the CF based algorithms
are very slow.

<<echo=FALSE>>=
load("xresults.rda")
@

<<eval=FALSE>>=
## this needs to be saved to disk! see run_recommender.R
algorithms <- list(
        "random items" = list(name="RANDOM", param=NULL),
        "popular items" = list(name="POPULAR", param=NULL),
        "user-based CF" = list(name="UBCF", param=list(method="Jaccard", nn=50)),
        "item-based CF" = list(name="IBCF", param=list(method="Jaccard", k=50)),
        "association rules" = list(name="AR", param=list(supp=0.001, conf=0.2, maxlen=2))
        )

## run algorithms
results <- evaluate(scheme, algorithms, n=c(1, 3, 5, 10, 15, 20))
## save(results, file="xresults.rda")
@

The result is an object of class~\class{evaluationResultList} for the
five recommender algorithms.
<<>>=
results
@

Individual results can be accessed by list subsetting using an index or 
the name specified when calling \func{evaluate}.

<<>>=
names(results)
results[["association rules"]]
@


Again \func{plot} can be used to create ROC and precision-recall plots
(see Figures~\ref{fig:roc2} and \ref{fig:precrec2}). Plot accepts most
of the usual graphical parameters like pch, type, lty, etc. In addition
annotate can be used to annotate the points on selected curves with the 
list length.

<<roc2, fig=TRUE, include=FALSE>>=
plot(results, annotate=c(1,3), legend="right")
@
\begin{figure}
\centerline{\includegraphics[scale=1]{recommenderlab-roc2}}
\caption{Comparison of ROC curves for several recommender methods for the 
given-3 evaluation scheme.}
\label{fig:roc2}
\end{figure}

<<precrec2, fig=TRUE, include=FALSE>>=
plot(results, "prec/rec", annotate=3)
@
\begin{figure}
\centerline{\includegraphics[scale=1]{recommenderlab-precrec2}}
\caption{Comparison of precision-recall curves 
for several recommender methods for the 
    given-3 evaluation scheme.}
\label{fig:precrec2}
\end{figure}

For this data set and the given evaluation scheme the 
user-based and item-based
CF methods clearly
outperform all other methods. In Figure~\ref{fig:roc2} we see that they
dominate the other method since for each length of top-$N$ list they
provide a better combination of TPR and FPR.

For comparison we will check how the algorithms compare given less information
using instead of a given-3 a given-1 scheme.

<<>>=
scheme1 <- evaluationScheme(MSWeb5, method="cross", k=4, given=1)
scheme1
@

<<eval=FALSE>>=
results1 <- evaluate(scheme1, algorithms, n=c(1,3,5,10,15,20))
## save(results1, file="xresults1.rda")
@
<<echo=FALSE>>=
load("xresults1.rda")
@

<<roc3, fig=TRUE, include=FALSE>>=
plot(results1, annotate=c(1,3), legend="right")
@
\begin{figure}
\centerline{\includegraphics[scale=1]{recommenderlab-roc3}}
\caption{Comparison of ROC curves for several recommender methods for the given-1 evaluation scheme.}
\label{fig:roc3}
\end{figure}

From Figure~\ref{fig:roc3} we see that given less information, 
the performance of item-based CF suffers the most and the simple 
popularity based recommender performs almost a well as user-based CF and 
association rules.

Similar to the examples presented here, it is easy to compare different
recommender algorithms for different data sets or to compare
different algorithm settings (e.g.,
the influence of neighborhood formation using different distance measures or different neighborhood sizes).

%\marginpar{getting data in missing!}

%\subsection{Compare rankings}
%
%Top-$N$ recommendation lists can be considered rankings and can be represented
%as $N$-ary order relations ($\leq$) on the set of all items in the lists (items
%which do not occur in all lists are added to the end of the corresponding
%order).
%
%{\bf Average distance between methods:}
%E.g., the cardinality of the symmetric difference of two relations (the
%		number of tuples contained in exactly one of two relations) 
%{\small\citep{Hornik:2008}}
%
%Total number of tuples in relations: 8649 (for 93 items)
%
%	\begin{verbatim}
%	UB   IB   AR Pop N
%	UB       0 2555 2634  3317
%	IB    2555    0 1611  2052
%	AR    2634 1611    0  2636
%	Pop N 3317 2052 2636     0
%	\end{verbatim}
%
%\small
%\begin{verbatim}
%u_a = {Toy Story (1995), Air Force One (1997), 
%	Cop Land (1997), Michael (1996), Blade Runner (1982)}
%
%	UB IB AR Top N Consensus*
%	Contact (1997)                 1  4 25     4         4
%	Liar Liar (1997)               2 18 32    10         8
%	Conspiracy Theory (1997)       3 NA NA    NA        NA
%	Star Wars (1977)               4  1  1     1         1
%	Return of the Jedi (1983)      5  2  4     2         2
%	English Patient, The (1996)    6 26 41     6        12
%	Saint, The (1997)              7 NA NA    NA        NA
%	Fargo (1996)                   8  3 13     3         3
%	Mr. Holland's Opus (1995)      9 34 30    39        17
%	Scream (1996)                 10 20 35     7        10
%	\end{verbatim}
%
%	* Consensus method: Condorcet (minimizes the weighted sum 
%			of symmetric difference distance)
%
%


%\subsection{ROCR interface}

\subsection{Implementing a new recommender algorithm}

Adding a new recommender algorithm to \pkg{recommenderlab} 
is straight forward since
it uses a registry mechanism to manage the algorithms.
To implement the actual recommender algorithm
we need to implement a creator function which takes a training data set,
trains a model and provides a predict function which uses the
model to create recommendations for new data.
The model and the predict function are both encapsulated in an
object of class~\class{Recommender}. 

For example the creator function
in Table~\ref{table:newalg} is called \func{BIN\_POPULAR}. It uses 
the (training) data to create a model which is a simple list (lines 4--7 in
Table~\ref{table:newalg}). 
In this case the model is just a list of all items sorted in 
decreasing order of
popularity.
The second part (lines 9--22) is the predict function which 
takes the model, new data and
the number of items of the desired top-$N$ list as its arguments.
Predict used the model to compute recommendations for 
each user in the new data and
encodes them as an object of class~\class{topNList} (line 16).
Finally, the trained model and the predict function are returned as 
an object of class~\class{Recommender} (lines 20--21).
Now all that needs to be done is to register the creator function. In this case
it is called POPULAR and applies to binary rating data (lines 25--28).

To create a new recommender algorithm the code in Table~\ref{table:newalg}
can be copied. Then lines 5, 6, 20, 26 and 27 need 
to be edited to reflect the new method name and description.
Line 6 needs to be replaced by the new model. More complicated
models might use several entries in the list. Finally, 
lines 12--14 need to be replaced by the recommendation code.

\begin{table}
\caption{Defining and registering a new recommender algorithm.}
\label{table:newalg}
%\begin{verbatim}
\begin{Verbatim}[gobble=0, numbers=left]
## always recommends the top-N popular items (without known items)
BIN_POPULAR <- function(data, parameter = NULL) {

  model <- list(
    description = "Order of items by popularity",
    popOrder = order(colCounts(data), decreasing=TRUE)
  )

  predict <- function(model, newdata, n=10) {
    n <- as.integer(n)

    ## remove known items and take highest
    reclist <- lapply(LIST(newdata, decode= FALSE),
      function(x) head(model$popOrder[!(model$popOrder %in% x)], n))

    new("topNList", items = reclist, itemLabels = colnames(newdata), n = n)
  }

  ## construct recommender object
  new("Recommender", method = "POPULAR", dataType = "binaryRatingMatrix",
    ntrain = nrow(data), model = model, predict = predict)
}

## register recommender
recommenderRegistry$set_entry(
  method="POPULAR", dataType = "binaryRatingMatrix", fun=BIN_POPULAR, 
  description="Recommender based on item popularity (binary data)."
)
\end{Verbatim}
%\end{verbatim}
\end{table}


\section{Conclusion}
\label{sec:conclusion}

Being able to use automated recommender systems 
offers a big advantage for online
retailers and for many other applications. But
often no extensive data base of rating data is available and
it makes sense to think about using 0-1 data, which is in many
cases easier to obtain, instead.
Unfortunately there is only limited research on collaborative filtering 
based recommender systems using 0-1 data available.

In this paper we described the \proglang{R} 
extension package~\pkg{recommenderlab}
which is especially geared towards developing and testing
recommender algorithms for 0-1 data.
The package allows to create evaluation schemes following accepted 
methods and then use them to evaluate and compare recommender algorithms.
Adding new recommender algorithms to the package is facilitates using
a registry to manage the algorithms. 

\pkg{recommenderlab} currently includes several algorithms for 0-1 data,
however, the infrastructure is flexible enough to also 
extend to the more conventional non-binary rating data and its algorithms.
In the future we will add more and more of these algorithms to the package and
we hope that some algorithms will also 
be contributed by other researchers.

%Applying collaborative filtering methods to binary and 
%especially to 0-1 data is difficult because  
%the sparsity of data has an adverse effect on similarity 
%calculation/neighborhood formation.
%Also the meaning of $0$ in the data can be either of ``unknown'' or
%``dislike'' and thus it is not clear how to best handle it.
%Measures like the Jaccard index can be used to mitigate this 
%problem since they focus mainly on $1$s.
%Overall, the quality of recommendations is extremely data set dependent.

%Also evaluation is biased since the complete set of ``liked'' items 
%is unknown. Measured precision is only a lower bound to real precision.

%\begin{itemize}
%\item Many companies face heterogeneous data sources which can best be
%aggregated into binary data. 
%\end{itemize}

%\item Model ``dislike'' in binary data {\small\citep{Mobasher:2001}}.
%\item Account for fast changing products 
%    (e.g., vintages differ often significantly and change every year).
%\item Investigate recommender engines based on consensus rankings.

%
%\bibliographystyle{abbrvnat}
%\bibliography{recommenderlab}
%

\bibliographystyle{abbrvnat}
\bibliography{recommender,recom_talk,data,stuff,association_rules}

\end{document}

